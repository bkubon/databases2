{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41064b1f",
   "metadata": {},
   "source": [
    "Aufgabe 1: [15/120 Punkte] Nenne 5 **verschiedene** Anwendungen wo Key-Value-Datenbanken Vorteile gegenüber Relationalen Datenbanken haben, und beschreibe spezifisch (aber dennoch stichwortartig) für jeden Fall, was genau der jeweilige Vorteil ist (und: was potentielle Nachteile sind)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b58852aa-b46a-4778-a44a-ece05f75eb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Session-Daten \n",
    "- Vorteil: Key-Value-Datenbanken erlauben eine schnelle LEse- und Schreibgeschwindigkeit. Hierbei kann besipielsweise der Key, dass Session-Datum und der Value die aufgerufenen Webseite darestellen.\n",
    "- Nachteil: Eingeschränkte Abfragemöglichkeiten, wenn hierbei nach einer aufgerufenen Webseite gesucht wird und das Session-Datum nicht vorhanden ist.\n",
    "2. Online-Shopping-Profil mit persönlichen Präferenzen \n",
    "- Vorteil: Flexile Skalierbarkeit beim Hinzufügen von Prodilinformationen, sowie schenlle Schreibgeschwindigkeit neuer Prodilinformationen in den Key-abhängigen Valueblock\n",
    "- Nachteil: Bei Updates auf den Value-Block wird der komplette Value-Block geupdatet\n",
    "3. Cloud-Storage\n",
    "- Vorteile: Sind sehr gut skalierbar auch über Cluster hinweg über mehrere Nodes. Schnelle Schreib- und Lesezugriffe.\n",
    "- Nachteile: Eingeschränkte Abfragemöglichkeiten. Abfragen sind nur auf den Value einer Key-Value-Datenbank möglich. Fehlender Transaktionssupport\n",
    "4. Caching\n",
    "- Vorteile: Schnelle Schreib- und Lesezugriffe auf Daten möglich. Oft verwendete Daten sind dadurch schnell zugreifbar. Flexibilität in Bezug auf die Verwendung verschiedener Speichermedien oder die Anpassung der Caching-Strategie\n",
    "- Nachteile: \n",
    "5.Content-Delivery-Networks\n",
    "- Vorteile: Key-Value-Datenbanken können hier verwendet werden, um verschiednesten Content wie Bilder, Videos und Audiofiles schnell und einfach auszuliefern.\n",
    "- Nachteile: Natürlich sind auch hier die Abfragemöglichkeiten begrenzt. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa609eeb",
   "metadata": {},
   "source": [
    "Aufgabe 2: [15/120 Punkte] erkläre was denn nun wirklich die Hauptunterschiede zwischen einem Data Warehouse und einem Data Lake sind und gib Beispiele für Anwendungen in denen entweder ein Data Warehouse oder ein Data Lake die bessere Wahl ist\n",
    "\n",
    "Der große Unterschied zwischen einem Data Warehouse und einem Data Lake ist die Art der Daten, welche gespecihert werden. Ein Data Warehouse ist hierbei eher für die Speicherung von strukturierten Daten spezialisiert. Das ist auch am Beladeprozess des Data Warehouse, dem ETL-Prozess erkennbar. Hierbei werden erst die Daten aus den verschiedensten Umsystemen des Unternehmens extrahiert, dann in eine gewissen Datenstruktur transformiert bevor sie in das eigentlich Data Warehouse geladen werden. \n",
    "Ein Data Lake isz hierbei eher für die Speicherung von semistrukturierten und unstrukturierten Daten ausgelegt. Ein aussagekräftiges Beispiel mist hierbei auch eine angepasstes Beispiel des ETL-Beladeprozess. Hierbei wird der Transform-Schritt nachdem Load Schritt ausgeführt. Das bedeutet, dass die Daten in ihrem ursprünglichen Schema in das Zielsystem geladen werden. \n",
    "\n",
    "Ein Data Warehouse wird gerne von Versicherungen im Bereich der Berichterstattung und des Rechnungslegung eingesetzt. Hierbei wird eine genaue und strukturierte Historisierung der Daten benötigt, um Abschlussberichte fehlerfrei auszuliefern. Mit einer Data Warehouse Struktur lässt sich auch hervorragend Data Mining betreiben. \n",
    "\n",
    "Ein Data Lake lässt sich hervorragend einsetzen, um das Marketing vieler Unternehmen zu unterstützen. Augrunf der großen Anzahl an Daten aus verschiedneen Datenquellen lassen sich bei einer Analyse Modelle für die Abwanderungswahrscheinlickeit der Kunden bilden. Somit lasse sich gezielte Präventionsmaßnahmen implementieren, welche die Abwanderung der Kundschaft senkt.\n",
    "\n",
    "Im Bereich der Versicherung lassen sich mit Hilfe von Machine-Learning-Prozesses auf den immensen Daten des Data Lakes Cross- und Upselling-Potentiale wahrnehmen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9a4491",
   "metadata": {},
   "source": [
    "Aufgabe 3: [15/120 Punkte] im folgenden Code hat sich der Fehlerteufel eingeschlichen. Finde die 5 Fehler (10 Punkte), und beschreibe stichwortartig, was der Code denn tun würde, wenn keine Fehler drin wären. (5 Punkte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6334810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bson.son import SON\n",
    "client = MongoClient(\"localhost\", 27017)\n",
    "connect = client[\"database\"],[\"collection\"]\n",
    "\n",
    "pipe2 = [ { '$mismatch' : { 'origin' : 'ATL',\n",
    "                         'dest' : 'BOS',\n",
    "                         'dayofweek' : 3\n",
    "                       }\n",
    "          },\n",
    "          { '$group' : { '_id' : { 'origin' : '$origin',\n",
    "                                   'destination' : '$dest'\n",
    "                                 },\n",
    "                         'Failure' : { '$sum' : { '$cond' : [{ '$eq' : ['$cancelled', 1]}, 1, 0 ]} },\n",
    "                         'Success' : { '$sum' : { '$cond' : [{ '$eq' : ['$cancelled', 0]}, 1, 0 ]} },\n",
    "                         'Total' : { '$sum' : 1 }\n",
    "                        }\n",
    "           },\n",
    "           { '$project' : { 'Failure' : 1,\n",
    "                            'Success' : 1,\n",
    "                            'Total' : 1,\n",
    "                            'FailPercent' : { '$divide' : [ '$Failure', '$Total' ] }\n",
    "                          }\n",
    "           },\n",
    "           # im SON-Statement ist kein Fehler, das gehoert so \n",
    "           { '$sort' : SON([('_id.origin', 1), ('_id.destination', 1 )]) }\n",
    "         \n",
    "\n",
    "result = connection.aggregate(pipeline=pipe2)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b64ee-edcb-49ee-a131-0fed677d0317",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. In the line connect = client[\"database\"],[\"collection\"], it appears that you want to connect to the \"database\" database and the \"collection\" collection, but the way this line is written it would create a tuple where the first element is the \"database\" database and the second element is a list containing the string \"collection\". To connect to a specific collection, you should use the MongoClient's get_database() method to get the database object and then call the get_collection() method on that object to get the collection object.\n",
    "\n",
    "2. In the line result = connection.aggregate(pipeline=pipe2), there is a typo. The variable name should be connect instead of connection.\n",
    "\n",
    "3. In the first stage of the pipeline { '$mismatch' : { 'origin' : 'ATL', 'dest' : 'BOS', 'dayofweek' : 3 } }, the operator used is '$mismatch' which is not a valid operator in MongoDB. You may want to use '$match' operator instead, which is used to filter the documents.\n",
    "\n",
    "4. The { 'FailPercent' : { '$divide' : [ '$Failure', '$Total' ] } } in the project stage is missing a closing parenthesis, which will cause a syntax error in your code.\n",
    "\n",
    "5. Es wird vergessen, dass Modul MongoClient zu importieren.\n",
    "\n",
    "This is Python code that uses the PyMongo library to connect to a MongoDB database. The code creates a MongoClient object that connects to a local MongoDB server running on port 27017.\n",
    "The connect variable is set to the \"database\" database and the \"collection\" collection. \n",
    "The pipe2 variable is a pipeline of four stages that is passed to the aggregate() method of the connection variable. \n",
    "The pipeline contains a $mismatch stage, a $group stage, a $project stage and a $sort stage. \n",
    "The pipeline filters the documents by origin, destination, and dayofweek, groups the documents by origin and destination, calculates the failure, success and total count, calculates the failure percentage and sorts the documents by origin and destination. \n",
    "The result variable stores the result of the aggregate() method and the print statement at the end of the code displays the result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ae06e21",
   "metadata": {},
   "source": [
    "Aufgabe 4: [15/120 Punkte] Erkläre was lazy evaluation ist an Hand von 2 in den Vorlesungen behandelten Technologien/Ansätzen (1 Lazy, eine nicht lazy) - und erläutere, was die Auswirkungen in der Praxis sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de412d84-bd62-4e8e-b52b-902dff0a9289",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Warum ist die *aggregation* einer Pipeline langsamer als der *find*-Befehl?\n",
    "• *find* = Iterator (Lazy-Evalutation), d.h. es wird nur der aktuelle Eintrag betrachtet\n",
    "• *aggregation* = Jeder Eintrag wird in eine Liste hinzugefügt, wobei die Liste jedesmal um\n",
    "    einen Eintrag erweitert wird und somit immer umfangreicher und größer wird"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e322834c",
   "metadata": {},
   "source": [
    "Aufgabe 5: [40/120 Punkte]\n",
    "    In Moodle findest Du Super-Markt Daten. Werte diese in Spark aus, und finde heraus:\n",
    "    \n",
    "    1) wie viele Einkäufe wurden von Frauen, bzw von Männern getätigt?\n",
    "    2) wie viel Umsatz wurde an den 3 Standorten jeweils getätigt - gib diesen zusammen mit dem NAMEN des jeweiligen Standorts aus\n",
    "    3) welche Bezahlart die höchste durchschnittliche User-Satisfaction hat\n",
    "    \n",
    "Exportiere die Ergebnisse von Teilaufgabe 3 als CSV-Datei - stelle dabei sicher, dass unabhängig von der Größe des Datensatzes nur 1 einzige CSV-Datei generiert wird.\n",
    "\n",
    "Beschreibe für jede der Teilaufgaben wie Du die Daten dazu aufbereiten würdest (Stichwort: Designentscheidungen)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "13b74806-0f41-4edd-8554-8907848be931",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|   Payment|count|\n",
      "+----------+-----+\n",
      "|creditcard|   81|\n",
      "|      cash|   84|\n",
      "|   ewallet|   90|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# Create SparkSession \n",
    "spark = SparkSession.builder \\\n",
    "      .master(\"local[1]\") \\\n",
    "      .appName(\"Name der DB\") \\\n",
    "      .getOrCreate() \n",
    "\n",
    "# Einlesen von Dateien in Spark, Datei Typ einfach Anpassen\n",
    "test=authors = spark.read.csv('supermarket_sales.csv', header=True)\n",
    "df5 = test.withColumn(\"Payment\", when(test.Payment == \"Creditcard\", \"creditcard\")\n",
    "                                .when(test.Payment == \"credit card\", \"creditcard\")\n",
    "                                .when(test.Payment == \"Credit card\", \"creditcard\")\n",
    "                                .when(test.Payment == \"Ewallet\", \"ewallet\")\n",
    "                                .when(test.Payment == \"Cash\", \"cash\")\n",
    "                                .otherwise(test.Payment))\n",
    "df6 = df5.groupBy(\"Payment\").count()\n",
    "df6.show()\n",
    "df6.write.option(\"header\",True) \\\n",
    " .csv(\"user_satisfaction_2.csv\")\n",
    "\n",
    "df1 = test.withColumn('year', split(test['Date'], '/').getItem(2)) \\\n",
    "       .withColumn('month', split(test['Date'], '/').getItem(0)) \\\n",
    "       .withColumn('day', split(test['Date'], '/').getItem(1))\n",
    "df1.show()\n",
    "\n",
    "df2 = df1.withColumn(\"month\", when(df1.month == \"1\", \"Januar\")\n",
    "                             .when(df1.month == \"2\", \"Februar\")\n",
    "                             .when(df1.month == \"3\", \"März\")\n",
    "                             .when(df1.month == \"4\", \"April\")\n",
    "                             .when(df1.month == \"5\", \"Mai\")\n",
    "                             .when(df1.month == \"6\", \"Juni\")\n",
    "                             .when(df1.month == \"7\", \"Juli\")\n",
    "                             .when(df1.month == \"8\", \"August\")\n",
    "                             .when(df1.month == \"9\", \"September\")\n",
    "                             .when(df1.month == \"10\", \"Oktober\")\n",
    "                             .when(df1.month == \"11\", \"November\")\n",
    "                             .when(df1.month == \"12\", \"Dezember\")\n",
    "                             .otherwise(df1.month))\n",
    "df2.show()\n",
    "\n",
    "df3 = df2.withColumn(\"Total\", df2[\"Unit price\"]*df2[\"Quantity\"])\n",
    "\n",
    "df3.show()\n",
    "\n",
    "df4 = df3.sort(\"Product line\").show(truncate=False)\n",
    "df4.show()\n",
    "\n",
    "df5 = test.withColumn(\"Branch\", when(test.Branch == \"A\", \"Yangon\")\n",
    "                                .when(test.Branch == \"B\", \"Mandalay\")\n",
    "                                .when(test.Branch == \"C\", \"Naypyitaw\")\n",
    "                                .otherwise(test.Branch))\n",
    "df6 = df5.groupBy(\"Branch\").count()\n",
    "df6.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5335ef0f",
   "metadata": {},
   "source": [
    "Aufgabe 6: [20/60 Punkte] Ein regionales Unternehmen hat ein großes Investment durch eine Venture Capital Firma bekommen, und möchte jetzt international expandieren. \n",
    "\n",
    "Euer Auftrag ist es das Unternehmen zum Thema \"Datenbanken\" (die für die internationale Expansion benötig werden) zu beraten.\n",
    "\n",
    "Das bisherige Hauptprodukt des Unternehmens sind mit Schokolade überzogene Fruchtgummis. Im Zuge der Expansion möchte das Unternehmen aber auch gleichzeitig neue Produkte einführen, bzw an regionale Geschmäcker anpassen. Dadurch, dass die Expansion überall gleichzeitig erfolgen soll, sind am Anfang wenig Möglichkeiten direkt Kundenumfragen zu machen - statt dessen soll die Resonanz auf Marketingcampagnen und die Interaktion mit der jeweiligen Landeswebseite genutzt werden, um lokale \"Geschmäcker\" und damit das Potential für weitere Produkte zu erforschen.\n",
    "\n",
    "Der bisher einzige Vertriebskanal ist eine Online-Präsenz, zumindest anfänglich soll dies auch so bleiben, jedoch mit potentiell separaten Webseiten für verschiedene Länder/Regionen. Hierzu besteht aber noch keine fest definierte Strategie, der Unternehmer würde von Euch auch gerne wissen, was für Möglichkeiten, Vor- und Nachteile, und Risiken verschiedener Ansätze (aus Sicht der Nutzung von Datenbanken) sind.\n",
    "\n",
    "Erläutere **stichwortartig**:\n",
    "\n",
    "1) welche Technologien in welchem Bereich eingesetzt werden, und warum\n",
    "2) was für Strategien, bzw Tools Ihr einsetzen wollt um \n",
    "    - die schon vorhandenen Daten zu migrieren\n",
    "    - neue Daten mit den alten zu verbinden\n",
    "    - benötigte Daten zu erhalten\n",
    "3) welche Strategien sich anbieten für das Reporting, die Datenhaltung, Backups etc.\n",
    "4) was die Auswirkungen verschiedener Expansionsstrategien (z.B. Marketing, Website etc) auf die Nutzung/Nützlichkeit/Performance der Datenbank(en) wäre \n",
    "5) Ob es \"Bottlenecks\" oder mit (hohem) Risiko verbundene Datenbank-Komponenten gibt, und wie man damit in der Praxis umgehen soll\n",
    "\n",
    "(wirklich nur stichwortartig, keine Romane!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9d6107-2686-41c9-95b9-3584064d29bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (99037639.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[1], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Das Unternehmen möchte international expandieren. Hier stellt sich die Frage, ob das Unternehmen neue eigene Ländergesellschaften gründet oder ob hierbei bestehende Unternehmen aufgekauft werden.\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Das Unternehmen möchte international expandieren. Hier stellt sich die Frage, ob das Unternehmen neue eigene Ländergesellschaften gründet oder ob hierbei bestehende Unternehmen aufgekauft werden. \n",
    "Je nachdem muss auf einer bestehenden Infratsruktur aufgbaut werden oder man kann die Infrastruktur nach seinen Wünschen an die Infrastruktur der bestehenden Ländergesellschaft anpassen.\n",
    "\n",
    "Um die Resonanz der Marketingkampagnen und die Interaktion des Landeswebseite zu Speichern bietet sich eine dokumentebasierte Datenbank an, welche zu einer gewissen Kundenkennung gewisse Parameter speichert.\n",
    "Dies kann je nach Interaktion durch den Kunden varieren. Bei einer eher geringen Datenmenge wie es in dem Fallbeispiel der Fall ist bietet sich für das Tracken der Nutzerinteraktion der Kunden ein Apache Kafka Installation an,\n",
    "mit Hilfe von Kafka Producern. Solange das Hosting der Webseiten von einem zentralen Standort erfolgt bietet sich für das Speichern der Daten ein Apache Cluster an, da hier Nachricvhten mit einem Zeitstempel gespeichert werden.\n",
    "Im Zuge dessen würde ich für die Verarbeitung der Daten auch bei Apache Kafka bleiben.\n",
    "\n",
    "Apache Kafka ermöglicht es auch alten Datensilos aufzubrechen, sodass die alten Daten aus den Legacy Sytemen mit den neuen Daten verbunden werden können, innerhalb eines Data Lakes.\n",
    "\n",
    "Ab einer gewissen Größe und gewissen regulatorischen Bedingungen bezüglich Rechnungslegung und Buchhaltung bietet es sich an die Daten der verschiedenen Ländergesellschaften in einem Data Warehouse bereitzustellen.\n",
    "Hierfür wird für die Normalisierung und Zusammenführung der Daten ein ETL-Tool gebraucht (s.h. SAS, PowerCenter). Gerade für die Anwendung in einem Data Warehouse bietet sich eine relationale Datenbank an (Bsp. Oracle, Postgres).\n",
    "Das Backup der Datenbank kann mit Hilfe von Apache Airflow auf Dateisystemebene erfolgen. Hierbei könnte natürlich auch ein CronJob mit den gewissen Bash Commands eingesetzt werden.\n",
    "\n",
    "Gerade in Bezug auf die Datenhaltung sind hierbei die regulatorischen Bedingungen der Länder der entsprechenden Ausstelle zu beachten. Beispielsweise müssen die Daten Schweizer Finanzunternehmen auch in der Schweiz gespeichert werden.\n",
    "\n",
    "Ein Bottleneck kann die bestehende Netzwerkverbindung zwischen den Zentralstandort und einem der internationalen Standorten sein."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
